---
layout: post
title: "Convolutional Architecture"
date: 2024-02-18
categories: Computer Vision
---

## Convolutional Architecture (From WashU CV Lecture 9 by Nathan Jacobs)

Important Terms :
- Three-dimensional convolutions
What if the input to a convolutional layer is a stack of K (Height x Width x K) filter and L filters. Assuming the input feature maps have a spatial resolution of HxW, and the filter has a size of FxF, we need total operations of $F^2$KLHW to compute the output feature volume. 
![](/images/2024-02-18/02.png)

- 1x1 convolutional layer
Instead of the filter being FxF, consider a 1x1 filter. (F = 1). Using a filter means extracting spatial features and mixing them across the channels. Therefore, 1x1 convolution purely mixes things across channels without extracting spatial features.
![](/images/2024-02-18/04.png)
The use of a 1x1 layer reduces costs and allows us to use less number of weights.

- Depthwise convolutions
![](/images/2024-02-18/05.png)
This is a description of one channel in, and one channel out. This cannot mix across channels and we are just extracting spatial features. We have separate filters for every input layer, therefore learning features of each layer. Due to the nature of Depthwise convolutions, we can use this with 1D convolution to mix across channels. Using depthwise with 1D convolution drastically decreases total number of operations needed as indicated by the slide below :
![](/images/2024-02-18/06.png)

- Groupwise convolutions
![](/images/2024-02-18/07.png)

- 3D to 2D convolutions
![](/images/2024-02-18/08.png)

- Backward pass
![](/images/2024-02-18/09.png)
![](/images/2024-02-18/10.png)
![](/images/2024-02-18/11.png)
![](/images/2024-02-18/12.png)
![](/images/2024-02-18/13.png)
![](/images/2024-02-18/14.png)

The backward pass, also known as backpropagation, is a key mechanism through which neural networks learn. It involves the computation of gradients (partial derivatives) of the loss function with respect to the weights of the network, and then using these gradients to update the weights in a direction that minimizes the loss. The process is iterative, with the goal of reducing the difference between the predicted output of the network and the actual target values over many epochs (full training cycles).

1. Forward Pass
Before the backward pass begins, the network performs a forward pass. In this phase, input data is passed through the network, layer by layer, until it reaches the output layer. At each layer, the network applies weights to the inputs, adds biases (if applicable), and then typically applies a non-linear activation function. The final output is used to compute the loss (or error) by comparing the network’s predictions against the actual target values using a loss function.

2. Computing the Loss
The **loss function measures how well the network’s predictions match the actual target values**. Common loss functions include Mean Squared Error (MSE) for regression tasks and Cross-Entropy Loss for classification tasks. The loss provides a single scalar value that summarizes the performance of the network on the input data.

3. Backward Pass (Backpropagation)
After the loss is computed, the network begins the backward pass to update its weights. The process involves the following steps:

Gradient of the Loss: First, the gradient of the loss function is calculated with respect to each weight in the network. This gradient indicates how a small change in each weight would affect the loss.

**Chain Rule of Calculus**: The backward pass leverages the chain rule of calculus to efficiently compute gradients for each layer, starting from the output layer and moving backward through the network. This is necessary because the *effect of weights on the loss involves a chain of operations across multiple layers*.

Update Weights: Once the gradients are computed, the weights are updated using an optimization algorithm such as Gradient Descent. The basic idea is to subtract a fraction of the gradient from the current weights, where the fraction is determined by a learning rate parameter. This process nudges the weights in a direction that reduces the loss. Stochastic Gradient Descent is often used over other iterative algorithms. 

Weight update formula: W_new = W_old - learning_rate * gradient

4. Iteration
The forward pass, loss computation, and backward pass are repeated for many iterations (epochs) over the training dataset. With each iteration, the network weights are adjusted to minimize the loss, ideally leading to improved performance on the task.
This text regarding backward pass is generated by GPT4. OpenAI. (2024). ChatGPT (4) [Large language model]. https://chat.openai.com

